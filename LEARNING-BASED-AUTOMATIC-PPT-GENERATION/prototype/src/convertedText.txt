



Artificial intelligence (AI) and increasingly complex algorithms currently influence our lives
and our civilization more than ever. The areas of AI application are diverse and the possi-
bilities extensive: in particular, because of improvements in computer hardware, certain
AI algorithms already surpass the capacities of human experts today. As AI capacity im-
proves, its field of application will grow further. In concrete terms, it is likely that the rel-
evant algorithms will start optimizing themselves to an ever greater degree—maybe even
reaching superhuman levels of intelligence. This technological progress is likely to present
us with historically unprecedented ethical challenges. Many experts believe that alongside
global opportunities, AI poses global risks, which will be greater than, say, the risks of nu-
clear technology—which in any case have historically been underestimated. Furthermore,
scientific risk analysis suggests that high potential damages should be taken very seriously
even if the probability of their occurrence were low.

Artificial Intelligence:
Opportunities and Risks

Policy paper
12 December 2015

Policy paper by the E(cid:27)ective Altruism Foundation.
First published (in German): 12 December 2015.
www.foundational-research.org
www.ea-sti(cid:28)ung.org

Preferred citation: Mannino, A., Althaus, D., Erhardt, J., Gloor, L., Hutter, A. and Metzinger, T. (2015).
Artificial Intelligence: Opportunities and Risks. Policy paper by the E(cid:27)ective Altruism Foundation (2):
1-16.

figure 3_1 ai_opportunities_and_risks
Contents

Executive Summary . . . . . . . . . . . . . . . . . . . . . . . 1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
Advantages and risks of current AIs . . . . . . . . . . . . . 3
Automation and unemployment . . . . . . . . . . . . . . . 5
General intelligence and superintelligence . . . . . . . . 7
Artificial consciousness . . . . . . . . . . . . . . . . . . . . . 9
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . 11
Supporters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . .

12

Adriano Mannino, Philosopher & Co-President, E(cid:27)ective
Altruism Foundation
David Althaus, Assistant Director, Foundational Research
Institute
Dr. Jonathan Erhardt, Scientific consultant, E(cid:27)ective
Altruism Foundation
Lukas Gloor, Researcher, Foundational Research Insti-
tute
Dr. Adrian Hutter, Physics Department, University of
Basel
Prof. Thomas Metzinger, Professor of Philosophy, Uni-
versity of Mainz


figure 5_1 ai_opportunities_and_risks
Artificial Intelligence: Opportunities and Risks
Measure 1 The promotion of a factual, rational discourse is essential so that cultural prejudices can be dismantled
and the most pressing questions of safety can be focused upon.
Measure 2 Legal frameworks must be adapted so as to include the risks and potential of new technologies. AI manu-
facturers should be required to invest more in the safety and reliability of technologies, and principles like predictabil-
ity, transparency, and non-manipulability should be enforced, so that the risk of (and potential damage from) unex-
pected catastrophes can be minimized.

Current
Mid-term

Executive Summary

Artificial intelligence (AI) and increasingly complex algorithms currently influence our lives and our civilization more than
ever before. The areas of AI application are diverse and the possibilities far-reaching, and thanks to recent improvements
in computer hardware, certain AI algorithms already surpass the capacities of today’s human experts. As AI capacity im-
proves, its field of application will continue to grow. In concrete terms, it is likely that the relevant algorithms will start opti-
mizing themselves to an ever greater degree and may one day attain superhuman levels of intelligence. This technological
progress is likely to present us with historically unprecedented ethical challenges. Many experts believe that, alongside
global opportunities, AI poses global risks surpassing those of e.g. nuclear technology (whose risks were severely under-
estimated prior to their development). Furthermore, scientific risk analyses suggest that high potential damages resulting
from AI should be taken very seriously—even if the probability of their occurrence were low.
In narrow, well-tested areas of application, such as driverless cars and certain areas of medical diagnostics, the superiority
of AIs over humans is already established. An increased use of technology in these areas o(cid:27)ers great potential, including
fewer road tra(cid:27)ic accidents, fewer mistakes in the medical treatment and diagnosing of patients, and the discovery of
many new therapies and pharmaceuticals. In complex systems where several algorithms interact at high speed (such as in
the financial market or in foreseeable military uses), there is a heightened risk that new AI technologies will be misused, or
will experience unexpected systematic failures. There is also the threat of an arms race in which the safety of technological
developments is sacrificed in favor of rapid progress. In any case, it is crucial to know which goals or ethical values ought
to be programmed into AI algorithms and to have a technical guarantee that the goals remain stable and resistant to
manipulation. With driverless cars, for instance, there is the well-known question of how the algorithm should act if a
collision with several pedestrians can only be avoided by endangering the passenger(s), not to mention how it can be
ensured that the algorithms of driverless cars are not at risk of hacking systematic failure.
Measure 1 The promotion of a factual, rational discourse is essential so that cultural prejudices can be dismantled
and the most pressing questions of safety can be focused upon.
Measure 2 Legal frameworks must be adapted so as to include the risks and potential of new technologies. AI manu-
facturers should be required to invest more in the safety and reliability of technologies, and principles like predictabil-
ity, transparency, and non-manipulability should be enforced, so that the risk of (and potential damage from) unex-
pected catastrophes can be minimized.
Progress in AI research makes it possible to replace increasing amounts of human jobs with machines. Many economists
assume that this increasing automation could lead to a massive increase in unemployment within even the next 10-20
years. It should be noted that while similar predictions in the past have proved inaccurate, the developments discussed
here are of a new kind, and it would be irresponsible to ignore the possibility that these predictions come true at some
point. Through progressive automation, the global statistical average living standard will rise; however, there is no guar-
antee that all people—or even a majority of people—will benefit from this.

Long-term

Artificial Intelligence: Opportunities and Risks

Measure 3 Can we as a society deal with the consequences of AI automation in a sensible way? Are our current social
systems su(cid:27)iciently prepared for a future wherein the human workforce increasingly gives way to machines? These
questions must be clarified in detail. If need be, proactive measures should be taken to cushion negative developments
or to render them more positive. Proposals like an unconditional basic income or a negative income tax are worth
examining as possible ways to ensure a fair distribution of the profits from increased productivity.
Many AI experts consider it plausible that this century will witness the creation of AIs whose intelligence surpasses that
of humans in all respects. The goals of such AIs could in principle take on any possible form (of which human ethical
goals represent only a tiny proportion) and would influence the future of our planet decisively in ways that could pose
an existential risk to humanity. Our species only dominates Earth (and, for better or worse, all other species inhabiting
it) because it currently has the highest level of intelligence. But it is plausible that by the end of the century, AIs will be
developed whose intelligence compares to ours as ours currently compares to, say, chimpanzees. Moreover, the possi-
bility cannot be excluded that AIs also develop phenomenal states—i.e. (self-)consciousness, and in particular subjective
preferences and the capacity for su(cid:27)ering—in the future,which would confront us with new kinds of ethical challenges. In
view of the immediate relevance of the problem and its longer-term implications, considerations of AI safety are currently
highly underrepresented in politics as well as research.
Measure 4 It is worth developing institutional measures to promote safety, for example by granting research funding
to projects which concentrate on the analysis and prevention of risks in AI development. Politicians must, in general,
allocate more resources towards the ethical development of future-shaping technologies.
Measure 5 E(cid:27)orts towards international research collaboration (analogous to CERN’s role in particle physics) are to
be encouraged. International coordination is particularly essential in the field of AI because it also minimizes the risk
of a technological arms race. A ban on all risky AI research would not be practicable, as it would lead to a rapid and
dangerous relocation of research to countries with lower safety standards.
Measure 6 Certain AI systems are likely to have the capacity to su(cid:27)er, particularly neuromorphic ones as they are
structured analogously to the human brain. Research projects that develop or test such AIs should be placed under
the supervision of ethical commissions (analogous to animal research commissions).

2

Introduction

The pursuit of knowledge runs as a governing principle
through human history. Whenever societies have under-
gone significant changes in their dynamics and structure,
this has normally been the result of new technological
inventions. Around two million years separate the first
use of stone tools from the historic moment when Homo
sapiens invented art and began to paint images on cave
walls. Another thirty thousand years passed before the
rise of arable farming and permanent settlement. The first
symbols appeared a few thousand years a(cid:28)er that, fol-
lowed closely by the first written scripts. Then, around
four hundred years ago, development began speeding up.
The microscope was invented in the seventeenth century;
industrialization in the nineteenth century enabled the
first cities of a million people; and during the last cen-
tury alone, the atom was split, humans set foot on the
Moon, and the computer was invented. Since then, the
processing capabilities and energy e(cid:27)iciency of computers
have doubled at regular intervals [1]. But while technolog-
ical progress o(cid:28)en develops exponentially, the same is not
true for human intellectual abilities.
In recent years, countless renowned scientists and en-
trepreneurs have warned of the urgent significance of AI,
and how important it is that policy makers tackle the chal-
lenges raised by AI research [2]. Exponents of this move-
ment for AI safety include Stuart Russell [3], Nick Bostrom
[4], Stephen Hawking [5], Sam Harris [6], Max Tegmark [7],
Elon Musk [8], Jann Tallinn [9] and Bill Gates [10].
In certain domain-specific areas, AIs have already
reached or even overtaken human levels on several oc-
Advantages and risks of current AIs
Our individual lives and our civilization as a whole are
governed to an ever-increasing extent by algorithms and
domain-specific artificial
intelligence (AIs) [19]. Well-
known examples include such ubiquitous things as smart-
phones, air tra(cid:27)ic control systems [20] and internet search
engines [21]. Financial markets, too, are dependent on al-
gorithms which are too large and complex for any single
human being to fully understand [22, 23]. The operation
of such algorithms, for the most part, proceed without in-
cident, but there is always the possibility that an unlikely
“black swan” event [24] might occur, threaten to plunge
the whole system into chaos. We have already witnessed
one such event: in 2010, an unexpected "flash crash" in

3

Artificial Intelligence: Opportunities and Risks
casions. In 1997 the computer Deep Blue beat the reign-
ing world champion Garry Kasparov at chess [11]; in 2011
Watson beat the two best human players on the language-
based game show Jeopardy! [12]; and in 2015 the first vari-
ant of poker, Fixed Limit Holdem heads-up, was game the-
oretically fully solved by Cepheus [13]. Meanwhile, artifi-
cial neural networks can compete with human experts in
the diagnosis of cancer cells [14] and are also more or less
approaching human levels in the recognition of handwrit-
ten Chinese characters [15]. Back in 1994, a self-learning
backgammon program reached the level of the world’s
best players by finding strategies that had never before
been played by humans [16]. By now, there even exist algo-
rithms that can independently learn many di(cid:27)erent games
from scratch and thereby reach (or surpass) human levels
[17, 18]. With these developments, we are slowly getting
closer to a general intelligence, which at least in principle
can solve problems of all sorts independently.
With great power comes great responsibility. Technol-
ogy is in itself just a tool; what matters is how we use it.
The use of existing AIs is already presenting us with consid-
erable ethical challenges, which will be illuminated in the
next section of this paper. The following chapter will out-
line developments in economic automation, and explain
the mid-term prognosis that AI research will give rise to a
significant restructuring of the labor market. Finally, the
two last chapters will discuss the long-term and existen-
tial risks of AI research in relation to the possible creation
of (super)human intelligence and artificial consciousness.
a US stock market le(cid:28) the financial world dumbfounded.
The crash occurred as a result of computer algorithms in-
teracting with the financial market in an unforeseen man-
ner [25, 26]. Within minutes, important shares lost more
than 90% of their worth and then quickly returned to their
high initial value.
If such an event were to take place in
a military context, a comparable “return to initial condi-
tions” would be improbable [27]. To prevent devastating
failures of this sort, it seems generally advisable to invest
considerably more resources into the safety and reliability
of AIs. Unfortunately, current economic incentives seem
to favor increased AI capacity far more than safety.

Artificial Intelligence: Opportunities and Risks
Four criteria for the construction of AIs
Safety is essential to the construction of any sort of ma-
chine. However, new ethical challenges arise when con-
structing domain-specific AIs capable of taking over cog-
nitive work in social dimensions—work that, until now,
has been carried out by humans. For instance, an al-
gorithm that judges the credit rating of bank customers
might make decisions that discriminate against certain
groups in the population (without this being explicitly pro-
grammed). Even technologies that simply replace exist-
ing actions could introduce interesting challenges for ma-
chine ethics [28]: driverless cars, for instance, raise the
question of which criteria should be decisive in the case of
an imminent accident. Should the vehicle ensure the sur-
vival of the passengers above all else or should it, in the
case of an unavoidable accident, prioritize keeping the to-
tal number of casualties as low as possible [29]?
Because of this, both AI theorist Eliezer Yudkowsky and
philosopher Nick Bostrom have suggested four principles
which should guide the construction of new AIs [30]: 1) the
functioning of an AI should be comprehensible and 2) its
actions should be basically predictable. Both of these cri-
teria must be met within a time frame that enables the re-
sponsible experts to react in time and veto control in case
of a possible failure. In addition, 3) AIs should be impervi-
ous to manipulation, and in case an accident still occurs, 4)
the responsibilities should be clearly determined.
Advantages of (domain specific) artificial intelligence
In principle, algorithms and domain-specific AIs bring
many advantages. They have influenced our lives for
the better and are expected to keep doing so at an ever-
increasing rate in the future, provided that the necessary
precautions are taken. Here we will discuss two instructive
examples.
Driverless cars are no longer science fiction [31, 32];
they’ll be commercially available in the foreseeable future.
The Google Driverless Car, which is driven completely by
autonomous AI algorithms, took its first test drive in the
USA back in 2011 [33, 34]. Besides the time gained for work
or relaxation, a second advantage to driverless cars con-
sists in their higher safety. In 2010, 1.24 million people died
worldwide in tra(cid:27)ic accidents, nearly exclusively because
of human error [35]. Countless human lives could there-
fore be saved every year, because driverless cars are al-
ready significantly safer than vehicles driven by humans
[36, 37].

Naturally, a large number of people remain skeptical

4

regarding driverless cars, mainly because they underesti-
mate the safety benefits thereof whilst at the same time
overestimating their own driving abilities. As an illustra-
tion of this latter point, one study came to the conclusion
that 93% of all American drivers believe that their driving
abilities are above the median [38]—which is statistically
impossible. Unrealistic optimism [39] and the illusion of
control [40] possibly also bias people towards underes-
timating the risks when they themselves are behind the
wheel [41, 42].
Doctors, too, overestimate their abilities [43], which in
the worst case can lead to deadly mishaps.
In the USA
alone, between an estimated 44,000 and 98,000 people
die each year in hospitals because of treatment mistakes
[44]. In this context, IBM’s Watson [45] is a welcome devel-
opment. This AI gained fame in 2011 when it beat the best
human players on the quiz show Jeopardy! [12]. Watson
isn’t just better than humans in quiz shows, however. Hos-
pitals have been able to hire Watson’s computing power
since 2014 for cancer diagnosis and other complex pattern-
recognition tasks. Because “Doctor Watson” can rapidly
collect and combine enormous quantities of information,
it has partially overtaken the diagnostic skills of its human
colleagues [46, 47].
The fact that a current AI can make more accurate med-
ical diagnoses than human doctors may seem surprising at
first, but it has long been recognized that statistical infer-
ences are superior to clinical judgments by human experts
in most cases [48, 49]. Seeing as AIs like Watson are ideal
for making statistical inferences, it follows that using com-
puters for certain types of diagnosis can save lives.
Cognitive biases: to err is human
One reason why human experts are less competent than
AIs at statistical inferences is the aforementioned (and, un-
fortunately, all too human) tendency to overestimate one’s
own abilities. This tendency is known as overconfidence
bias [50] and is just one of many documented cognitive bi-
ases that can lead to systematic errors in human thinking
[51, 52]. AIs, on the other hand, can be built so as to avoid
cognitive biases altogether. In principle, increasing confi-
dence in the predictions of AIs could lead to a significantly
more rational and e(cid:27)icient approach to many social and
political challenges, provided they are made safely and ac-
cording to comprehensible criteria. The problem here lies
in using the strengths of AI without at the same time giving
up human autonomy in the corresponding systems.

Conclusion and outlook

Irrational fears towards new and basically advantageous
technologies are widespread, both now and in the past
[53]. Such “technophobia” may also be one of the rea-
sons that Watson or driverless cars are met with skepti-
cism. However, being wary of kinds of technology is not al-
ways irrational. Most technologies can be used to the ben-
efit of humanity, but can also be dangerous when they fall
into the wrong hands, or when insu(cid:27)icient care is taken for
safety and unforeseen side e(cid:27)ects.
Automation and unemployment
In light of recent successes in the field of machine learning
and robotics, it seems there is only a matter of time until
even complicated jobs requiring high intelligence could be
comprehensively taken over by machines [56].
If machines become quicker, more reliable and cheaper
than human workers in many areas of work, this would
likely cause the labour market to be uprooted on a scale
not seen since the Industrial Revolution. According to
economists like Cowen [57], McAfee and Brynjolfsson [58],
technological progress will widen the income gap even fur-
ther and may lead to falling incomes and rising unemploy-
ment in large segments of the population.
A 2013 analysis concluded that it will likely be possible
to automate 47% of all jobs in the USA within 10-20 years
[59]. The hardest jobs to automate are those which require
high levels of social intelligence (e.g. PR consultation), cre-
ativity (e.g. fashion design) and/or sensitive and flexible
object manipulation (e.g. surgery). In these domains, the
state of AI research is still far below the level of human ex-
perts.
Advantages and disadvantages to automation by com-
puters
Those who will benefit the most from technological
progress are the people and nations that understand how
to make use of new technological opportunities and the
corresponding flood of “big data” [60]. In particular, coun-
tries with well-trained computer specialists are expected
to prosper in the face of technological progress. More-

Recommendation 1 — Responsible approach: As with all other technologies, care should be taken to ensure that the
(potential) advantages of AI research clearly outweigh the (potential) disadvantages. The promotion of a factual, ratio-
nal discourse is essential so that irrational prejudices and fears can be broken down. Current legal frameworks have
to be updated so as to accommodate the challenges posed by new technologies. The four principles described above
should be followed for every extensive use of AIs [30].
(cid:4)

5

Artificial Intelligence: Opportunities and Risks
This also holds for artificial intelligence: driverless
cars could make our lives easier and save human lives,
but complex computer algorithms can also cause the
stock market to crash unexpectedly. While the risks from
domain-specific AIs appear limited in the near future,
there are long-term developments to take into consider-
ation:
in the not-so-distant future, artificial intelligence
could in principle pose an existential threat, similar in
scope to the pandemic risks associated with biotechnol-
ogy [54, 55, 4].
over, it is likely that a thorough understanding of the ways
in which various computer algorithms compare to hu-
man decision-making and working abilities—as well as the
(dis)advantages of each—will become increasingly impor-
tant in the future, thus necessitating high standards of ed-
ucation [61].
Following the automation of the production and ser-
vice industries, one might expect only the entertainment
industry to remain; yet here, too, we are already witness-
ing extensive changes. With flawless computer graphics,
novel entertainment technologies, and countless smart-
phone apps all becoming increasingly a(cid:27)ordable, the ad-
dictive pull of videogames and internet usage is rising [62].
While we have not yet been able to research the long-term
social and psychological consequences of this develop-
ment, several factors currently indicate that these trends
are profoundly changing our social behavior [63], atten-
tion spans, and childhood development [64]. These ef-
fects may be amplified by the increasing use of virtual re-
ality technology, which is already available to consumers.
As these become increasingly detailed and realistic, they
may blur the user’s boundaries between reality and simu-
lation, thereby invading deeper into our everyday experi-
ence. The consequences of more regular immersion in vir-
tual realities—including experiences like body-transfer il-
lusions, in which subjective awareness is temporarily pro-
jected into a virtual avatar [65]—should receive greater at-
tention.
While the entertainment industry does o(cid:27)er significant

Utopias and dystopias

Artificial Intelligence: Opportunities and Risks
opportunities for better education through personalized
AI teaching and the gamification of learning material [66],
it also increases the risk that a growing proportion of
young people will have trouble completing their education
due to a pathological addiction to video games and/or the
internet [67].
Technological progress increases societal productivity
[68], in turn raising the average standard of living [69]. If
more work is carried out by machines, this frees up time for
leisure and self-development for humans—at least those
in a position to profit from it. However, a drawback to in-
creasing automation could be that the increases in pro-
ductivity go along with increasing social inequality so that
a rise in the mean standard of living doesn’t coincide with
a rise in the median quality of life. Experts like the MIT eco-
nomics professor Erik Brynjolfsson even worry that tech-
nological progress threatens to make the lives of a major-
ity of people worse [70].
In a competitive economy where AI technology has pro-
gressed to the point where many jobs are done by ma-
chines, the income for automatable human work will fall
[58]. Without regulation, the incomes of many people
could sink below subsistence level. Social inequality may
rise sharply if economic output were to increase more
rapidly than the wages needed to e(cid:27)ect redistribution.
To counteract this development, McAfee and Brynjolfsson
suggest that limiting certain jobs to humans should be
subsidized. Additional options for ensuring fair distribu-

Recommendation 2 — Forward thinking: As in the case of climate change, incentives should be set for researchers
and decision makers to deal with the consequences of AI research; only then can the foundations of precautionary
measures be laid. In particular, specialist conferences should be held on AI safety and on assessing the consequences
of AI, expert commissions should be formed, and research projects funded.
(cid:4)
Recommendation 3 — Education: The subsidization of human work, an unconditional basic income, and a negative
income tax have all been proposed as measures to cushion the negative social impacts of increased automation. Re-
search should be conducted toward finding additional options, as well as identifying which set of measures has the
maximum e(cid:27)ect. Moreover, advantages and disadvantages must be systematically analyzed and discussed at a politi-
cal level, and research grants should be established in order to answer any empirical questions that will inevitably arise
as a result of this discussion.
(cid:4)
Recommendation 4 — Transparency over new measures: The subsidisation of human work, an unconditional basic
income or a negative income tax have been proposed as measures to cushion the negative social impacts of increasing
automation. It is worth clarifying which further options exist and which set of measures has the maximum e(cid:27)ect. In
addition, advantages and disadvantages must be systematically analysed and discussed at a political level. Research
grants should be established to answer the empirical questions thrown up by this discussion.
(cid:4)

6

tion of advantages from technological progress amongst
the whole population include unconditional basic income,
and a negative income tax [71, 72]
Some experts also warn of future scenarios in which
the projected changes are even more drastic. For ex-
ample, the economist Robin Hanson expects that it
will be possible within this century to digitally run hu-
man brain simulations—so-called whole brain emulations
(WBEs) [73]—in virtual reality. WBEs would be repro-
ducible, and could (assuming that su(cid:27)icient hardware is
available) run many times faster than a biological brain,
consequently implying a huge increase in labor e(cid:27)iciency
[74]. Hanson predicts that in such a case, there would
be a “population explosion” amongst WBEs, who could
be used as enormously cost-e(cid:27)icient workers [75]. Han-
son’s speculations are contested [61], and it should not be
assumed that they sketch out the most likely future sce-
nario. Current research in this field, such as the Blue Brain
Project at ETH Lausanne, is still very far from the first brain
simulations—never mind supplying them in real time (or
even faster) with inputs from a virtual reality. However, it is
important to keep hardware developments in mind in rela-
tion to the possibility of WBEs. If the scenario sketched out
by Hanson were to occur, this would be of great ethical rel-
evance. For one thing, many humans replaced by complex
simulations could become unemployed; for another, there
is the question whether the WBEs deployed would have
phenomenal consciousness and subjective preferences—
in other words, whether they would experience su(cid:27)ering
as a result of their (potentially forced) labor.

General intelligence and superintelligence
General intelligence measures an agent’s ability to achieve
goals in a wide range of environments [76, 77]. This kind
of intelligence can pose a (catastrophic) risk if the goals of
the agent do not align with our own. If a general intelli-
gence reaches a superhuman level, it becomes a superin-
telligence; that is, an algorithm superior to human intel-
ligence in every way, including scientific creativity, “com-
mon sense”, and social competence. Note that this defini-
tion leaves open the question of whether or not a superin-
telligence would have consciousness [78, 79].
Comparative advantages of general artificial intelligence
over humans
Humans are intelligent, two-legged “bio-robots” possess-
ing a conscious self-awareness, and were developed over
billions of years of evolution. These facts have been used
argue that the creation of artificial intelligence may not be
so di(cid:27)icult, [80, 81, 82] seeing as AI research can be con-
ducted in a faster, more goal-oriented way than evolution
(which only progresses through the slow accumulation of
successive generations). Alongside the fact that evolution
is a precondition for the feasibility of AIs, it naturally also
permits directed human research to borrow from biologi-
cal design and thereby proceed considerably faster.
Compared to the biological brain of a person, computer
hardware o(cid:27)ers several advantages[4, p. 60]: the basic
computational elements (modern microprocessors) “fire”
millions of times faster than neurons; signals are trans-
mitted millions of times faster; and a computer can store
considerably more basic computational elements in total
(a single supercomputer can easily take up an entire fac-
tory floor). A future digital intelligence would also have big
advantages over the human brain in relation to so(cid:28)ware
components [4, pp. 60–61]: for instance, it is easy to both
modify and multiply, meaning that potentially relevant in-
formation can be called upon at any time. In a few impor-
tant areas such as energy e(cid:27)iciency, resilience to purely
physical damage, and graceful degradation [83], artificial
hardware still lags behind the human brain. In particular,
there is still no direct relation between thermodynamic ef-
ficiency and complexity reduction at the level of informa-
tion processing [84, 85], but this may change as computer
hardware improves in coming decades.
In view of these comparative advantages and the pre-
dicted rapid improvement of hardware [86] and so(cid:28)ware,
it seems probable that human intelligence will someday
be overtaken by that of machines. It is important to assess

Timeframes
7

Goals of a general intelligence

Artificial Intelligence: Opportunities and Risks
more precisely how and when this could take place, and
where the implications of such a scenario lie.
Di(cid:27)erent experts in the area of AI have considered the
question of when the first machines will reach the level of
human intelligence. A survey of the hundred most success-
ful AI experts, measured according to a citation index, re-
vealed that a majority consider it likely that human-level
AI will be developed within the first half of this century [4,
p. 19]. The belief that humans will create a superintelli-
gence by the end of this century, as long as technologi-
cal progress experiences no large setbacks (as a result of
global catastrophes), was also held by the majority of ex-
perts [4, p. 20]. The variance among these estimates is
high: some experts are confident that there will be ma-
chines with at least human levels of intelligence no later
than 2040; (fewer) other experts think that this level will
never be reached. Even if one makes a somewhat conser-
vative assumption, accounting for the tendency of human
experts to be overconfident in their estimates [87, 88], it
would still be inappropriate to describe superintelligence
as mere “science fiction” in the light of such widespread
confidence among relevant experts.
As a rational agent, an artificial intelligence strives towards
just what its goals/goal function describes [89]. Whether
an artificial intelligence will act ethically, that is, whether
it will have goals which are not in conflict with the interests
of humans and other sentient beings, is completely open:
an artificial intelligence can in principle follow all possi-
ble goals [90].
It would be a mistaken anthropomorphi-
sation to think that every kind of superintelligence would
be interested in ethical questions like (typical) humans.
When we build an artificial intelligence, we also establish
its goals, explicitly or implicitly.
These claims are sometimes criticized on the grounds
that any attempt to direct the goal of an artificial intelli-
gence according to human values would amount to “en-
slavement,” because our values would be forced upon the
AI [91]. However, this criticism rests on a misunderstand-
ing, as the expression “forced” suggests that a particular,
“true” goal already exists, one the AI has before it is cre-
ated. This idea is logically absurd, because there is no
pre-existing agent “receiving” the goal function in the first

Artificial Intelligence: Opportunities and Risks
place, and thus no goal independent of the processes that
have created an agent. The process that creates an intel-
ligence determines inevitably its functioning and goals. If
we intend to build a superintelligence, then we, and noth-
ing and nobody else, are responsible for its goals. Fur-
thermore, it is also not the case that an AI must experi-
ence any kind of harm through the goals that we inevitably
give it. The possibility of being harmed in an ethically rele-
vant sense requires consciousness, which we must ensure
is not achieved by a superintelligence. Parents inevitably
form the values and goals of their children’s “biological in-
telligence” in a very similar way, yet this does obviously
not imply that children are thereby “enslaved” in an un-
ethical manner. Quite the opposite: we have the greatest
ethical duty to impart fundamental ethical values to our
children. The same is true for the AIs that we create.
The computer science professor Stuart Russell warns
that the programming of ethical goals poses a great chal-
lenge [3], both on a technical level (how would complex
goals in a programming language be written so that no un-
foreseen consequences resulted?) and on an ethical level
(which goals anyhow?). The first problem is called the
value-loading problem in the literature [92].
Although the scope of possible goals of a superintel-
ligence is huge, we can make some reliable statements
about the actions they would take. There is a range of in-
strumentally rational subgoals that are useful for agents
with highly varied terminal goals. These include goal- and
self-preservation, increasing one’s intelligence, and re-
source accumulation [93]. If the goal of an AI were altered,
this could be as negative (or even more so) to the achieve-
ment of its original goal as the destruction of the AI itself.
Increased intelligence is essentially just an ability to reach
goals in a wider range of environments, and this opens
up the possibility of a so-called intelligence explosion, in
which an AI rapidly undergoes an enormous increase in its
intelligence through recursive self-improvement [94, 95]
(a concept first described by I.J. Good [96] which has since
been formalized in concrete algorithms [97].) Resource ac-
cumulation and the discovery of new technologies give the
AI more power, which in turn serves better goal achieve-
ment. If the goal function of a newly developed superintel-
ligence ascribed no value to the welfare of sentient beings,
it would cause reckless death and su(cid:27)ering wherever this
was useful for its (interim) goal achievement.
One could tend towards the assumption that a super-
intelligence poses no danger because it is only a com-
puter, which one could literally unplug. By definition, how-
ever, a superintelligence would not be stupid; if there were

What is at stake
Rational risk management
8

any probability that it would be unplugged, a superintelli-
gence could initially behave itself as the makers wished it
to, until it had found out how to minimize the risk of an in-
voluntary shutdown [4, p. 117]. It could also be possible for
a superintelligence to circumvent the security systems of
big banks and nuclear weapon arsenals using hitherto un-
known gaps in security (so-called zero day exploits), and in
this way to blackmail the global population and force it to
cooperate. As mentioned earlier, in such a scenario a “re-
turn to the initial situation” would be highly improbable.
In the best-case scenario, a superintelligence could solve
countless problems for humanity, helping us overcome
the greatest scientific, ethical, ecological and economic
challenges of the future. If, however, the goals of a super-
intelligence were incompatible with the preferences of hu-
man beings or any other sentient beings, it would amount
to an unprecedented existential threat, potentially caus-
ing more su(cid:27)ering than any preceding event in the known
universe [98].
In decision situations where the stakes are very high, the
following principles are of crucial importance:
1. Expensive precautions can be worth the cost even
for low-probability risks, provided there is enough to
win/lose thereby [89].
2. When there is little consensus in an area amongst ex-
perts, epistemic modesty is advisable. That is, one
should not have too much confidence in the accu-
racy of one’s own opinion either way.
The risks of AI research are of a global nature.
If AI re-
searchers fail to transfer ethical goals to a superintelli-
gence in the first attempt, there quite possibly won’t be
a second chance. It is absolutely tenable to estimate the
long-term risks of AI research as even greater than those
of climate change. In comparison to climate change, how-
ever, AI research is receiving very little attention. With this
paper, we want to emphasize that it is therefore even more
valuable to invest considerable resources into AI safety re-
search.
If the scenarios discussed here have a non-infinitesimal
chance of actually happening, then artificial intelligence
and the opportunities and risks associated with it should
be a global priority. The probability of a good outcome of
AI research can be maximized through a number of mea-
sures, including the following: If the scenarios discussed

Artificial consciousness

here have (a perhaps small, but) more than an infinites-
imal chance of actually happening, then artificial intelli-
gence and the opportunities and risks associated with it
Humans and many non-human animals have what is
known as phenomenal consciousness—that is, they expe-
rience themselves to be a human or a non-human animal
with a subjective, first-person point of view [99]. They
have sensory impressions, a (rudimentary or pronounced)
sense of self, experiences of pain upon bodily damage,
and the capacity to feel psychological su(cid:27)ering or joy (see
for example the studies of depression in mice [100]).
In
short, they are sentient beings. Consequently, they can be
harmed in a sense that is relevant to their own interests
and perspective. In the context of AI, this leads to the fol-
lowing question: Is it possible for the functional system of
a machine to also experience a potentially painful “inner
life”? The philosopher and cognitive scientist Thomas Met-
zinger o(cid:27)ers four criteria for the concept of su(cid:27)ering, all of
which would apply to machines as well as animals:

Recommendation 5 — Information: An e(cid:27)ective improvement in the safety of artificial intelligence research begins
with awareness on the part of experts working on AI, investors, and decision-makers. Information on the risks asso-
ciated with AI progress must, therefore, be made accessible and understandable to a wide audience. Organizations
supporting these concerns include the Future of Humanity Institute (FHI) at the University of Oxford, the Machine In-
telligence Research Institute (MIRI) in Berkeley, the Future of Life Institute (FLI) in Boston, as well as the Foundational
Research Institute (FRI).
(cid:4)
Recommendation 6 — AI safety: Recent years have witnessed an impressive rise in investment into AI research [86],
but research into AI safety has been comparatively slow. The only organization currently dedicated the theoretical
and technical problems of AI safety as its top priority is the aforementioned MIRI. Grantors should encourage research
projects to document the relevance of their work to AI safety, as well as the precautions taken within the research itself.
At the same time, high-risk AI research should not be banned, as this would likely result in a rapid and extremely risky
relocation of research to countries with lower safety standards.
(cid:4)
Recommendation 7 — Global cooperation and coordination: Economic and military incentives create a competitive
environment in which a dangerous AI arms race will almost certainly arise. In the process, the safety of AI research
will be reduced in favor of more rapid progress and reduced cost. Stronger international cooperation can counter this
dynamic. If international coordination succeeds, then a “race to bottom” in safety standards (through the relocation of
scientific and industrial AI research) would also be avoided.

2. A phenomenal self-model.

3. The ability to register negative value (that is, violated
4. Transparency (that is, perceptions feel irrevocably

1. Consciousness.

subjective preferences) within the self-model.
4. Transparency (that is, perceptions feel irrevocably

9

Artificial Intelligence: Opportunities and Risks
should be a global priority. The probability of a good out-
come of AI research can be maximised through the follow-
ing measures, amongst others:
“real”, thus forcing the system to self-identify with
the content of its conscious self-model) [101, 102].
Two related questions have to be distinguished actually:
firstly, whether machines could ever develop conscious-
ness and the capacity for su(cid:27)ering at all; and secondly, if
the answer to the first question is yes, which types of ma-
chines (will) have consciousness.
In addition to the above, two related questions have
to be distinguished: Firstly, whether machines could tech-
nically develop consciousness and the capacity for su(cid:27)er-
ing at all; Secondly, if the answer to the first question is
yes, which types of machines (will) have consciousness.
These two questions are being researched by philosophers
and AI experts alike. A glance at the state of research re-
veals that the first question is easier to answer than the
second. There is currently substantial, but not total, con-
sensus amongst experts that machines could in principle
have consciousness, and that it is at least possible in neu-
romorphic computers [103, 104, 105, 106, 107, 108, 109].
Such computers have hardware with the same functional
organization as a biological brain [110]. The question of
identifying which types of machines (besides neuromor-

(cid:4)

Artificial Intelligence: Opportunities and Risks
phic computers) could have consciousness, however, is far
more di(cid:27)icult to answer. The scientific consensus in this
area is less clear [111]. For instance, it is disputed whether
pure simulations (such as the simulated brain of the Blue
Brain Project) could have consciousness. While some ex-
perts are confident that this is the case [109, 105], others
disagree [111, 112].
In view of this uncertainty among experts, it seems rea-
sonable to take a cautious position: According to current
knowledge, it is at least conceivable that many su(cid:27)iciently
complex computers, including non-neuromorphic ones,
could be sentient.
These considerations have far-reaching ethical conse-
quences.
If machines could have consciousness, then it
would be ethically unconscionable to exploit them as a
workforce, and to use them for risky jobs such as defusing
mines or handling dangerous substances [4, p. 167]. If suf-
ficiently complex AIs will have consciousness and subjec-
tive preferences with some probability, then similar ethi-
cal and legal safety precautions to those used for humans
and non-human animals will have to be met [113]. If, say,
the virtual brain of the Blue Brain Project was to gain con-
sciousness, then it would be highly ethically problematic
to use it (and any potential copies or “clones”) for system-
atic research of e.g. depression by placing it in depres-
Already today, we are witnessing the spread of novel AI
technologies with surprising potential. The AI technology
currently behind driverless cars, Watson-assisted medical
diagnosing, and US military drones will gradually become
available for general use in the foreseeable future.
It is
crucial that carefully constructed legal frameworks are in
place before this happens, so as to realize the potential of
these technologies in ways that safely minimize any risks

Conclusion

Recommendation 8 — Research: In order to make ethical decisions, it is important to have an understanding of which
natural and artificial systems have the capacity for producing consciousness, and in particular for experiencing su(cid:27)er-
ing. Given the apparent level of uncertainty and disagreement within the field of machine consciousness, there is a
pressing need to promote, fund, and coordinate relevant interdisciplinary research projects (comprising philosophy,
neuroscience, and computer science).
(cid:4)
Recommendation 9 — Regulation: It is already standard practice for ethics commissions to regulate experiments on
living test subjects [114, 115]. In light of the possibility that neuromorphic computers and simulated beings could also
develop consciousness, it is vital that research on these, too, is carried out under the strict supervision of ethics commis-
sions. Furthermore, the (unexpected) creation of sentient artificial life should be avoided or delayed wherever possible,
as the AIs in question could—once created—be rapidly duplicated on a vast scale. In the absence of pre-existing legal
representation and political interest in artificial sentience, this proliferation would likely continue unchecked.
(cid:4)

10

sive circumstances. Metzinger warns that conscious ma-
chines could be misused for research purposes. Moreover,
as “second class citizens”, they may lack legal rights and be
exploited as dispensable experimental tools, all of which
could be negatively reflected at the level of the machines’
inner experience [106]. This prospect is particularly worry-
ing because it is conceivable that AIs will be made in such
huge numbers [4, 75] that in a worst-case scenario, there
could be an astronomical number of victims, outnumber-
ing any known catastrophe in the past.
These dystopian scenarios point toward an important
implication of technological progress: Even if we make
only “minor” ethical mistakes (e.g. by erroneously clas-
sifying certain computers as unconscious or morally in-
significant), then by virtue of historically unprecedented
technological power, this could result in equally unprece-
dented catastrophes. If the total number of sentient be-
ings rises drastically, we must ensure that our ethical val-
ues and empirical estimates improve proportionally; a
mere marginal improvement in either parameter will be
insu(cid:27)icient to meet the greatly increased responsibility.
Only by acknowledging the uncertain nature of possible
machine consciousness can we begin to take appropri-
ate cautionary measures in AI research, and thus hope to
avoid any of the potential catastrophes described above.
of a negative overall development.
The more progress is made in the field of AI technol-
ogy, the more pressing a rational, far-sighted approach to
the associated challenges becomes. Because political and
legal progress tends to lag behind technological develop-
ment, there is an especially large amount of responsibility
resting on the individual researchers and developers who
directly take part in any progress being made.

Supporters

Acknowledgements

We thank all those who helped us in the research or writing of this position paper. Worthy of particular mention are: Kaspar
Etter and Massimo Mannino, for their suggestions on the structure of the paper; professor Oliver Bendel, for suggestions
to the chapter “Advantages and risks of current AIs”; and professor Jürgen Schmidhuber, both for his inputs to the chap-
ters “General intelligence and superintelligence” and “Artificial consciousness” and for his valuable contributions to the
current state of knowledge in various fields of AI research.

Unfortunately, however, there are strong economic in-
centives for the development of new technologies to take
place as fast as possible without “wasting” time on expen-
sive risk analyses. These unfavorable conditions increase
the risk that we gradually lose our grip on the control of
AI technology and its use. This should be prevented on
all possible levels, including politics, the research itself,
and in general by anyone whose work is relevant to the is-
sue. A fundamental prerequisite to directing AI develop-
ment along the most advantageous tracks possible will be
The central points of this position paper are supported by:

• Prof. Dr. Fred Hamker, Professor of Artificial Intelligence, Technical University of Chemnitz
• Prof. Dr. Dirk Helbing, Professor of Computational Social Science, ETH Zürich
• Prof. Dr. Malte Helmert, Professor of Artificial Intelligence, University of Basel
• Prof. Dr. Manfred Hild, Professor of Digital Systems, Beuth Technical College, Berlin
• Prof. Dr. Dr. Eric Hilgendorf, Director of Research in Robotic Law, University of Würzburg
• Prof. Dr. Marius Klo(cid:28), Professor of Machine Learning, Humboldt University, Berlin
• Prof. Dr. Jana Koehler, Professor of Information Science, Luzern College
• Prof. Dr. Stefan Kopp, Professor of Social Cognitive Systems, University of Bielefeld
• Prof. Dr. Dr. Franz Josef Radermacher, Professor of Databases and Artificial Intelligence, University of Ulm

11

Artificial Intelligence: Opportunities and Risks
to broaden the field of AI safety. This way, it can be recog-
nized not only among a few experts but in widespread pub-
lic discourse as a great (perhaps the greatest) challenge of
our age.
As a final addition to the concrete recommendations
given above, we would like to conclude by pleading that AI
risks and opportunities be recognized as a global priority—
akin to climate change, or the prevention of military
conflicts—as soon as possible.

figure 16_1 ai_opportunities_and_risks
Bibliography

[1] Koomey, J. G., Berard, S., Sanchez, M., & Wong, H. (2011). Implications of Historical Trends in the Electrical E(cid:27)iciency
[2] Brockman, J. (2015). What to Think About Machines That Think: Today’s Leading Thinkers on the Age of Machine
[3] Russell, S. (2015). Will They Make Us Better People? (http://edge.org/response-detail/26157)
[4] Bostrom, N. (2014). Superintelligence: Paths, Dangers, Strategies. Oxford University Press.
[5] BBC. (2015a). Stephen Hawking Warns Artificial Intelligence Could End Mankind. (http://www.bbc.com/news/
[6] Harris, S. (2015). Can We Avoid a Digital Apocalypse? (https://edge.org/response-detail/26177)
[7] The Independent. (2014). Stephen Hawking: ‘Transcendence Looks at the Implications of Artificial Intelligence —
But Are We Taking AI Seriously Enough?’ (http://www.independent.co.uk/news/science/stephen- hawking-
transcendence-looks-at-the-implications-of-artificial-intelligence--but-are-we-taking-ai-seriously-enough-
9313474.html)
[8] The Guardian. (2014). Elon Musk Donates $10m to Keep Artificial Intelligence Good for Humanity. (http://www.
[9] SBS. (2013). Artificial Irrelevance: The Robots Are Coming. (http://www.sbs.com.au/news/article/2012/07/18/
[10] BBC. (2015b). Microso(cid:28)’s Bill Gates Insists AI Is a Threat. (http://www.bbc.com/news/31047780)
[11] Silver, N. (2012). The Signal and the Noise: Why So Many Predictions Fail – But Some Don’t. Penguin.
[12] PCWorld. (2011). IBM Watson Vanquishes Human Jeopardy Foes. (http://www.pcworld.com/article/219893/ibm_
[13] Bowling, M., Burch, N., Johanson, M., & Tammelin, O. (2015). Heads-up Limit Hold’em Poker Is Solved. Science,
[14] Ciresan, D. C., Giusti, A., Gambardella, L. M., & Schmidhuber, J. (2013). Mitosis Detection in Breast Can-
cer Histology Images Using Deep Neural Networks. MICCAI 2013. (http : / / people . idsia . ch / ~juergen /
deeplearningwinsMICCAIgrandchallenge.html)
[15] Ciresan, D., Meier, U., & Schmidhuber, J. (2012). Multi-Column Deep Neural Networks for Image Classification. Com-
[16] Tesauro, G. (1994). TD-Gammon, a Self-Teaching Backgammon Program, Achieves Master-Level Play. Neural Com-
[17] Koutník, J., Cuccu, G., Schmidhuber, J., & Gomez, F. (2013). Evolving Large-Scale Neural Networks for Vision-Based
Reinforcement Learning. In Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation
(pp. 1061–1068). ACM.
[18] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... Ostrovski, G. et al. (2015). Human-
[19] Slavin, K. (2012). How Algorithms Shape Our World. (http://ed.ted.com/lessons/kevin-slavin-how-algorithms-

of Computing. IEEE Annals of the History of Computing, 33(3), 46–54.
watson_vanquishes_human_jeopardy_foes.html)
puter Vision and Pattern Recognition 2012, 3642–3649.
Level Control Through Deep Reinforcement Learning. Nature, 518(7540), 529–533.

Intelligence. Harper Perennial.
technology-30290540)
putation, 6(2), 215–219.

artificial-irrelevance-robots-are-coming)
watson_vanquishes_human_jeopardy_foes.html)

theguardian.com/technology/2015/jan/16/elon-musk-donates-10m-to-artificial-intelligence-research)
Level Control Through Deep Reinforcement Learning. Nature, 518(7540), 529–533.

347(6218), 145–149.
putation, 6(2), 215–219.
shape-our-world)

[20] Tagesanzeiger. (2008). Computer-Panne legt US-Flugverkehr lahm. (http : / / www . tagesanzeiger. ch / ausland /
[21] Page, L., Brin, S., Motwani, R., & Winograd, T. (1999). The PageRank Citation Ranking: Bringing Order to the Web.
[22] Wired. (2010). Algorithms Take Control of Wall Street. (http://www.wired.com/2010/12/(cid:27)_ai_flashtrading/all/)
[23]
[24] Taleb, N. N. (2010). The Black Swan: The Impact of the Highly Improbable Fragility. Random House.
[25]
[26] Securities, U., Commission, E., & the Commodity Futures Trading Commission. (2010). Findings Regarding the Mar-
ket Events of May 6, 2010. Report of the Sta(cid:27)s of the CFTC and SEC to the Joint Advisory Committee on Emerging
Regulatory Issues.
[27] Spiegel. (2015). Denkende Wa(cid:27)en: Künstliche-Intelligenz-Forscher Warnen vor Künstlicher Intelligenz. (http : / /
www.spiegel.de/netzwelt/netzpolitik/elon- musk- und- stephen- hawking- warnen- vor- autonomen- wa(cid:27)en-
a-1045615.html)
[28] Bendel, O. (2013). Towards Machine Ethics. In Technology Assessment and Policy Areas of Great Transitions (pp. 343–
[29] Goodall, N. J. (2014). Machine Ethics and Automated Vehicles. In Road Vehicle Automation: Lecture Notes in Mobility
[30] Bostrom, N. & Yudkowsky, E. (2013). The Ethics of Artificial Intelligence. In Cambridge Handbook of Artificial Intelli-
[31] Dickmanns, E. D., Behringer, R., Dickmanns, D., Hildebrandt, T., Maurer, M., Thomanek, F., & Schiehlen, J. (1994).
[32] Dickmanns, E. (2011). Evening Keynote: Dynamic Vision as Key Element for AGI. 4th Conference on Artificial General
[33] Thrun, S. (2011). Google’s Driverless Car. (http://www.ted.com/talks/sebastian_thrun_google_s_driverless_car)
[35] Organization, W. H. et al. (2013). WHO Global Status Report on Road Safety 2013: Supporting a Decade of Action. World
[36] Simonite, T. (2013). O(cid:27)line Handwriting Recognition with Multidimensional Recurrent Neural Networks. MIT Tech-
[37] CNBC. (2014). Self-Driving Cars Safer Than Those Driven by Humans: Bob Lutz. (http : / / www . cnbc . com / id /
[38] Svenson, O. (1981). Are We All Less Risky and More Skillful Than Our Fellow Drivers? Acta Psychologica, 9(6), 143–
[39] Weinstein, N. D. (1980). Unrealistic Optimism about Future Life Events. Journal of Personality and Social Psychology,
[40]
[41] Von Hippel, W. & Trivers, R. (2011). The Evolution and Psychology of Self-Deception. Behavioral and Brain Sciences,
[42] Trivers, R. (2011). The Folly of Fools: The Logic of Deceit and Self-Deception in Human Life. Basic Books.
[43] Berner, E. S. & Graber, M. L. (2008). Overconfidence as a Cause of Diagnostic Error in Medicine. The American Journal

[34]

Lauricella, T. & McKay, P. (2010). Dow Takes a Harrowing 1,010.14-point Trip. Wall Street Journal (May 7, 2010).
The Seeing Passenger Car ‘VaMoRs-P’. In International Symposium on Intelligent Vehicles 94 (pp. 68–73).
Intelligence, Mountain View, CA. (https://www.youtube.com/watch?v=YZ6nPhUG2i0)
Forbes. (2012). Nevada Passes Regulations for Driverless Cars. (http://www.forbes.com/sites/alexknapp/2012/02/
17/nevada-passes-regulations-for-driverless-cars/)
39(5), 806.
Langer, E. J. (1975). The Illusion of Control. Journal of Personality and Social Psychology, 32(2), 311.

(http://ilpubs.stanford.edu:8090/422/)
(pp. 93–102). Springer International Publishing.
gence. Cambridge University Press.

amerika/ComputerPanne-legt-USFlugverkehr-lahm/story/13800972)
Lin, T. C. (2012). The New Investor. UCLA L. Rev. 60, 678–735.
347). Proceedings from the PACITA 2013 Conference in Prague.
Intelligence, Mountain View, CA. (https://www.youtube.com/watch?v=YZ6nPhUG2i0)

Health Organization.
nology Review, Oct, 25.
of Medicine, 121(5), S2–S23.

101981455)
34(1), 1–56.

148.

13

Artificial Intelligence: Opportunities and Risks

Artificial Intelligence: Opportunities and Risks

[44] Kohn, L. T., Corrigan, J. M., Donaldson, M. S. et al. (2000). To Err Is Human: Building a Safer Health System. National
[45] The New York Times. (2010). What Is IBM’s Watson? (http://www.nytimes.com/2010/06/20/magazine/20Computer-
[46] Wired. (2013). IBM’s Watson Is Better at Diagnosing Cancer Than Human Doctors. (http://www.wired.co.uk/news/
[48] Dawes, R. M., Faust, D., & Meehl, P. E. (1989). Clinical Versus Actuarial Judgment. Science, 243(4899), 1668–1674.
[49] Grove, W. M., Zald, D. H., Lebow, B. S., Snitz, B. E., & Nelson, C. (2000). Clinical Versus Mechanical Prediction: A
[50] West, R. F. & Stanovich, K. E. (1997). The Domain Specificity and Generality of Overconfidence: Individual Di(cid:27)er-
[51] Tversky, A. & Kahneman, D. (1974). Judgment Under Uncertainty: Heuristics and Biases. Science, 185(4157), 1124–
[52] Pohl, R. (Ed.). (2004). Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory.
[53] Brosnan, M. J. (2002). Technophobia: The Psychological Impact of Information Technology. Routledge.
[54]
[55] Bostrom, N. (2002). Existential Risks. Journal of Evolution and Technology, 9(1).
[56] Smith, A. & Anderson, J. (2014). AI, Robotics, and the Future of Jobs. Pew Research Center.
[57] Cowen, T. (2013a). Average Is Over: Powering America Beyond the Age of the Great Stagnation. Penguin.
[58] Brynjolfsson, E. & McAfee, A. (2014). The Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant
[60] Helbing, D. (2015). Thinking Ahead — Essays on Big Data, Digital Revolution, and Participatory Market Society.
[61] Cowen, T. (2013b). EconTalk Episode with Tyler Cowen: Tyler Cowen on Inequality, the Future, and Average is Over.
[62] Gri(cid:27)iths, M., Kuss, D., & King, D. (2012). Video Game Addiction: Past, Present and Future. Current Psychiatry Reviews,
[63] Srivastava, L. (2010). Mobile Phones and the Evolution of Social Behaviour. Behavior & Information Technology,
[64] Prensky, M. (2001). Do They Really Think Di(cid:27)erently? On the Horizon, 47(2).
[65] Metzinger, T. (2015a). Virtuelle Verkörperung in Robotern. SPEKTRUM, 2, 48–55.
[66] Kapp, K. M. (2012). The Gamification of Learning and Instruction: Game-Based Methods and Strategies for Training
[67] Bavelier, D., Green, S., Hyun Han, D., Renshaw, P., Merzenich, M., & Gentile, D. (2011). Viewpoint: Brains on Video

[47]
[59]

Forbes. (2013). IBM’s Watson Gets Its First Piece Of Business In Healthcare. (http : / / www . forbes . com / sites /
bruceupbin/2013/02/08/ibms-watson-gets-its-first-piece-of-business-in-healthcare/)
Yudkowsky, E. (2008). Artificial Intelligence as a Positive and Negative Factor in Global Risk. Global Catastrophic
Risks, 1, 303.
Technologies. WW Norton & Company.
Frey, C. B. & Osborne, M. A. (2013). The Future of Employment: How Susceptible Are Jobs to Computerisation?
Oxford Martin Programme on Technology and Employment. (https://web.archive.org/web/20150109185039/http:
//www.oxfordmartin.ox.ac.uk/downloads/academic/The_Future_of_Employment.pdf)

ences in Performance Estimation Bias. Psychonomic Bulletin & Review, 4(3), 387–392.
(http://www.econtalk.org/archives/2013/09/tyler_cowen_on.html)

t.html)
1131.
Springer.

Academies Press.
Psychology Press.
8(4), 308–318.
24(2), 111–129.
and Education. Pfei(cid:27)er.

archive/2013-02/11/ibm-watson-medical-doctor)
Meta-Analysis. Psychological Assessment, 12(1), 19.
Games. Nature Reviews Neuroscience, 12, 763–768.

14

[68]

[69] Galor, O. & Weil, D. N. (1999). From Malthusian Stagnation to Modern Growth. American Economic Review, 150–154.
[70] Brynjolfsson, E. (2014). EconTalk Episode with Erik Brynjolfsson: Brynjolfsson on the Second Machine Age. (http:
[71] Hughes, J. J. (2014). Are Technological Unemployment and a Basic Income Guarantee Inevitable or Desirable? Jour-
[72] Krugman, P. (2013). Sympathy for the Luddites. New York Times, 13. (http://www.nytimes.com/2013/06/14/opinion/
[73] Bostrom, N. & Sandberg, A. (2008). Whole Brain Emulation: A Roadmap. Oxford: Future of Humanity Institute.
[74] Hanson, R. (2012). Extraordinary Society of Emulated Minds. (http://library.fora.tv/2012/10/14/Robin_Hanson_
[77] Hutter, M. (2007). Universal Algorithmic Intelligence: A Mathematical Top-Down Approach. In Artificial General In-
[78] Bostrom, N. (1998). How Long Before Superintelligence? International Journal of Future Studies, 2.
[79] Schmidhuber, J. (2012). Philosophers & Futurists, Catch Up! Response to The Singularity. Journal of Consciousness
[80] Moravec, H. (1998). When Will Computer Hardware Match the Human Brain. Journal of Evolution and Technology,
[81] Moravec, H. (2000). Robot: Mere Machine to Transcendent Mind. Oxford University Press.
[82] Shulman, C. & Bostrom, N. (2012). How Hard Is Artificial Intelligence? Evolutionary Arguments and Selection E(cid:27)ects.
[83] Sengupta, B. & Stemmler, M. (2014). Power Consumption During Neuronal Computation. Proceedings of the IEEE,
[84]
[85] Sengupta, B., Stemmler, M., & Friston, K. (2013). Information and E(cid:27)iciency in the Nervous System — A Synthesis.
[86] Eliasmith, C. (2015). On the Eve of Artificial Minds. In T. Metzinger & J. M. Windt (Eds.), Open mind. MIND Group.
[87] Armstrong, S., Sotala, K., & ÓhÉigeartaigh, S. S. (2014). The Errors, Insights and Lessons of Famous AI Predictions
— And What They Mean for the Future. Journal of Experimental & Theoretical Artificial Intelligence, 26(3), 317–342.
[88] Brenner, L. A., Koehler, D. J., Liberman, V., & Tversky, A. (1996). Overconfidence in Probability and Frequency Judg-
[89] Peterson, M. (2009). An Introduction to Decision Theory. Cambridge University Press.
[90] Armstrong, S. (2013). General Purpose Intelligence: Arguing the Orthogonality Thesis. Analysis and Metaphysics,
[91] Noë, A. (2015). The Ethics Of The ‘Singularity’. (http://www.npr.org/sections/13.7/2015/01/23/379322864/the-
[92] Bostrom, N. (2012). The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents.

[75] Hanson, R. (1994). If Uploads Come First. Extropy, 6(2), 10–15.
[76]

Fagerberg, J. (2000). Technological Progress, Structural Change and Productivity Growth: A Comparative Study.
Structural Change and Economic Dynamics, 11(4), 393–411.
Legg, S. & Hutter, M. (2005). A Universal Measure of Intelligence for Artificial Agents. In International Joint Confer-
ence on Artificial Intelligence (Vol. 19, p. 1509). Lawrence Erlbaum Associates ltd.
102(5), 738–750.
Friston, K. (2010). The Free-Energy Principle: A Unified Brain Theory? Nature Reviews Neuroscience, 11, 127–138.
ments: A Critical Examination. Organizational Behavior and Human Decision Processes, 65(3), 212–219.

1(1), 10.
(12), 68–84.

Studies, 19(1-2), 173–182.
PLoS Comput Biol, 9(7).
ethics-of-the-singularity)

//www.econtalk.org/archives/2014/02/brynjolfsson_on.html)
Journal of Consciousness Studies, 19(7-8), 103–130.
(http://open-mind.net/papers/@@chapters?nr=12)

nal of Evolution and Technology, 24(1), 1–4.
krugman-sympathy-for-the-luddites.html)
Extraordinary_Society_of_Emulated_Minds)
telligence (Vol. 6, 2, pp. 227–290). Springer.
Journal of Consciousness Studies, 19(7-8), 103–130.
(http://open-mind.net/papers/@@chapters?nr=12)
Minds and Machines, 22(2), 71–85.

15

Artificial Intelligence: Opportunities and Risks

Artificial Intelligence: Opportunities and Risks

[93] Omohundro, S. M. (2008). The Basic AI Drives. In Proceedings of the First AGI Conference, 171, Frontiers in Artificial
[94] Solomono(cid:27), R. (1985). The Time Scale of Artificial Intelligence: Reflections on Social E(cid:27)ects. Human Systems Man-
[95] Chalmers, D. (2010). The Singularity: A Philosophical Analysis. Journal of Consciousness Studies, 17(9-10), 7–65.
[96] Good, I. J. (1965). Speculations Concerning the First Ultraintelligent Machine. In Advances in Computers (pp. 31–88).
[97] Schmidhuber, J. (2006). Gödel Machines: Fully Self-Referential Optimal Universal Self-Improvers. In Artificial Gen-
[98] Tomasik, B. (2011). Risks of Astronomical Future Su(cid:27)ering. Foundational Research Institute. (http://foundational-
[99] Nagel, T. (1974). What Is it Like to Be a Bat? The Philosophical Review, 435–450.
[100] Durgam, R. (2001). Rodent Models of Depression: Learned Helplessness Using a Triadic Design in Tats. Curr Protoc
[101] Metzinger, T. (2012). Two Principles for Robot Ethics. In H. E & G. J-P (Eds.), Robotik und Gesetzgebung (pp. 263–302).
NOMOS. (http://www.blogs.uni-mainz.de/fb05philosophie/files/2013/04/Metzinger_RG_2013_penultimate.pdf)
[102] Metzinger, T. (2015b). Empirische Perspektiven aus Sicht der Selbstmodell-Theorie der Subjektivität: Eine Kurzdarstel-
lung mit Beispielen. Selbstverlag. (http://www.amazon.de/Empirische-Perspektiven-Sicht-Selbstmodell-Theorie-
Subjektivitat-ebook/dp/B01674W53W)
[103] Moravec, H. P. (1988). Mind Children: The Future of Robot and Human Intelligence. Harvard University Press.
[104] Chalmers, D. J. (1995). Absent Qualia, Fading Qualia, Dancing Qualia. Conscious Experience, 309–328.
[105] Chalmers, D. J. (1996). The Conscious Mind: In Search of a Fundamental Theory. Oxford University Press.
[106] Metzinger, T. (2010). The Ego Tunnel: The Science of the Mind and the Myth of the Self (First Trade Paper Edition). New
[107] Metzinger, T. (2015c). What If They Need to Su(cid:27)er? (https://edge.org/response-detail/26091)
[108] Dennett, D. C. (1993). Consciousness Explained. Penguin UK.
[109] Bostrom, N. (2003). Are We Living in a Computer Simulation? The Philosophical Quarterly, 53(211), 243–255.
[110] Hasler, J. & Marr, B. (2013). Finding a Roadmap to Achieve Large Neuromorphic Hardware Systems. Frontiers in
[111] Koch, C. (2014). What it Will Take for Computers to Be Conscious, MIT Technology Review. (http : / / www .
[112] Tononi, G. (2015). Integrated Information Theory. Scholarpedia, 10(1), 4164. (http://www.scholarpedia.org/article/
[113] Singer, P. (1988). Comment on Frey’s ‘Moral Standing, the Value of Lives, and Speciesism’. Between the Species: A
[114] Swissethics, Verein anerkannter Ethikkommissionen der Schweiz. (n.d.). (http://www.swissethics.ch/)
[115] Senatskommission für Tierexperimentelle Forschung. (2004). Tierversuche in der Forschung. (http://www.dfg.
de / download / pdf / dfg _ im _ profil / geschae(cid:28)sstelle / publikationen / dfg _ tierversuche _ 0300304 . pdf, pub-
lisher=Deutsche Forschungsgemeinscha(cid:28))

agement, 5, 149–153.
Academic Press.
Neurosci, (8).
York: Basic Books.
Neuroscience, 7(118).
Journal of Ethics, 4, 202–203.

eral Intelligence (pp. 119–226).
Neuroscience, 7(118).
Integrated_Information_Theory)
Journal of Ethics, 4, 202–203.

research.org/publications/risks-of-astronomical-future-su(cid:27)ering/)

Intelligence and Applications (Vol. 171, pp. 483–492).

technologyreview.com/news/531146/what-it-will-take-for-computers-to-be-conscious/)

16


www.foundational-research.org

www.ea-sti(cid:28)ung.org
© 2016

